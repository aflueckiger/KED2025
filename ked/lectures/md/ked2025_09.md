---
title: The ABC of Computational Text Analysis
subtitle: "#9 Ethics and the Evolution of NLP"
author: "Alex Fl√ºckiger"
institute: "Faculty of Humanities and Social Sciences<br>University of Lucerne" 
date: "1 May 2025"
lang: en-US
---

## Recap last lecture

-   2nd assignment accomplished ‚úÖ
-   reminder: [share mini-project idea](https://docs.google.com/spreadsheets/d/1p9ufvzEKvhm7j_Q6EiawdcJDIFpdGHYvTQC_i_dP9aU/edit?gid=0#gid=0) by today ‚ùó
-   perform first real-world (data) analysis üìä
    -   analyse discourse on wokeness in Swiss media


::: notes
-   assignment korrigiert
    -   alle ans Ziel gekommen, mehr inhaltliche Auseinandersetzung gew√ºnscht
    -   Musterl√∂sung aufgeschaltet
    -   manche haben Screenshots, statt Python File hochgeladen ‚Äì\> Assignment erg√§nzt wie man Speicherort findet von Explorer aus
-   Datenanalyse
    -   Datensatz zu Wokeness von TA und NZZ
    -   Analyse von Metadaten, weniger Text selbst
    -   Publikationsdynamik: neuer Diskurs, NZZ arbeitet sich am Thema ab
:::

## Outline

-   evaluate this course üì£
-   ethics is everywhere üôà üôâüôä
    -   ... and your responsibility
-   understand the development of modern NLP üöÄ
    -   ... or how to put words into computers

::: notes
-   heute ein Sprung, der √ºber praktischen Teil von Seminar hinausgeht
    -   Mix aus Ethik und Entwicklung aktueller NLP
    -   moderne NLP leistungsf√§higer als je zuvor, aber mit Problemen
-   AI ist Werkzeug, erstmal weder gut noch schlecht
    -   Vergleich I: Motor f√ºr Krankenwagen oder Panzer (Verteidigung/Angriff).
    -   Vergleich II: Internet f√ºr Wikipedia oder f√ºr Kinderpornographie
    -   Es sind wir Menschen, die die Technologie kontrollieren
:::

## Course Evaluation {data-background-image="../images/speech_bubbles.gif"}

## Tell me... üì£ {data-background="var(--blue)"}

### Please follow the QR code or link in the email received

![](../images/qr_code_eval.png)

<br>

### Thanks for any constructive feedback, <br>be it sweet or sour! üôè

::: notes
-   offene Kommentare nutzen, statt nur Kreuze
-   \>5min Zeit zum Ausf√ºllen
-   andere Code anschauen
-   Tell me...
    -   ... what you disliked
    -   ... what you missed
    -   ... what you learned
:::

# **Ethics** is more than an academic subject.<br>It **is everywhere**.

::: notes
- Ethik ist nicht nur abstrakt, geh√∂rt nicht nur in Philosophie
- praktische Relelanz demonstrieren
- starten mit einem modernen, aber realistischen Beispiel
:::



# Apply for a job at a big company üéì üìÑ üè¢

::: notes
-   Anekdoten aus eigenem Bewerbungsprozess
    -   als Bewerber
    -   f√ºr Jobs, die Tools zur automatischen CV-Verarbeitung machen
:::

# ü´µ {data-background-image="../images/cv.png"}


# Does your CV pass the automatic pre-filtering? <br> üî¥ üü¢
::: notes
-   automatische Vorselektion Bewerbungen
-   Aus welchen Gr√ºnden vorselektiert oder eben auch nicht? --> Intransparent
-   bestenfalls: ausgewogen, aber naiv; schlechtestenfalls: diskriminierend
:::

# Your interview is recorded. üï∂Ô∏è ü•µ
::: notes
- Dann gehts weiter, n√§chste Stufe im Interviewprozess
:::

## What personality is inferred automatically?

![Facial expressions as perceived by a model by [@Peterson2022]](../images/face_impressions.jpg){width="80%"}

::: notes
-   Smartness korreliert mit Alter und Haarfarbe
-   Trustworthyness korrelliert mit L√§cheln und sonnengebr√§unter Haut
-   Doch was sagen √Ñusserlichkeiten tats√§chlich √ºber psychologische Eigenschaften aus?
    -   Viel Pseudowissenschaft, auch von Topuniversit√§ten
:::

## Don't worry about the future...

![The narrative of autonomous and evil-minded robots is an illusion.](../images/terminator_robot.jpg)

::: notes
- Das Problem mit AI liegt nicht in einer Zukunft, in der uns Menschen pl√∂tzlich Roboter angreifen
- sondern in der Gegenwart
:::

## ...worry about the present üòü {data-background="var(--red)"} 

-   AI is persuasive in everyday life
    -   assessing risks and performances (credit, job, crime, terrorism etc.) [@Hofmann2024]
-   AI is extremely capable
    -   increasingly difficult to assess where it fails
-   AI has data-driven bias
    -   systems are often evaluated poorly

::: notes
-   je mehr die Systeme k√∂nnen, 
    -   desto mehr werden sie eingesetzt
    -   desto unsichtbarer wird, was sie nicht k√∂nnen
-   Moderne AI lernt Muster aus unseren Daten.
    -   Existierende Ungleichheit wird reproduziert
        -   Geschlecht, Ethnie, sozio√∂konomisch
-   Problematik ist Reichweite, Intransparenz, Systematik
    -   Menschen auch fallibel, aber mit gr√∂sser Variet√§t
    -   Dinge werden pl√∂tzlich entscheidbar, die vorher ad hoc waren
-   Anwendungen f√ºr tabellarische Daten, Text, Bild, Video
    -   self-driving cars (kid vs adult, walking vs wheel chair)

-   Entwicklung NLP und Ethik h√§ngen zusammen
    -   quasi: je leistungsf√§higer NLP, desto mehr Bias wird mitgelernt im aktuellen Paradigma
    -   besseres Verst√§ndnis = bessere Data Science
-   Daher machen wir Sprung in die Entwicklung von NLP
:::

# An (r)evolution of NLP {.white-text data-background-image="../images/words.jpg"}

## What is a word?

-   words \~ segments between whitespace
-   yet, there are ...
    -   contractions: `U.S.`, `don't`
    -   collocations: `New York`

## Token

::::: columns
::: {.column width="50%"}
-   token \~ computational unit
    -   representation of words
-   lemma \~ base form of a word
    -   `texts` ‚Üí¬†`text`
    -   `goes` ‚Üí¬†`go`
-   stop words \~ functional words
    -   lacking deeper meaning
    -   `the`, `a`, `on`, `and` ...
:::

::: {.column width="50%"}
![Segmenting a text into tokens](../images/tokenization.jpeg)
:::
:::::

<br>

`Let's tokenize this sentence! Isn't is easy?` ü§ì

::: notes
-   Text wird in seine Teile gesplittet
-   Lemma \~ Form in Duden
-   Stopwords wie in Assignment auch bemerkt
:::

## Classic processing steps in NLP

::::: columns
::: {.column width="50%"}
1.  Tokenizing
    -   segmenting text into words, punctuation etc.
2.  Tagging part-of-speech (POS)
    -   assigning word types (e.g. verb, noun)
3.  Parsing
    -   describing syntactic relations
4.  Named Entity Recognition (NER)
    -   organizations, persons, locations, time etc.
:::

::: {.column width="50%"}
![Automatically inferred information of a sentence](../images/spacy_parse.png)
:::
:::::

<br> üëâ Catch up on NLP with <br>@Jurafskyforthcominga

::: notes
-   Abfolge von Prozessierungsschritten

    -   erinnert bisschen an Deutschunterricht

    -   linguistische Modellierungen

-   alles sprachabh√§ngig

-   Tagging: Wortartenbestimmung

-   Parsing

    -   Verb ist Head von Satz
    -   Subjekt-Verb-Objekt
:::

# How to represent words in computers?

::: notes
-   Wie werden im Hintergrund W√∂rter, Text und ganze Textsammlungen repr√§sentiert?
:::

## From bag of words to embeddings

### Putting words into computers <br> [@Smith2020; @Church2021; @Manning2022]

-   how to measure similarity of words and documents?
-   from **counting** to rich **semantic representations**


::: notes
-   Sprache hat ein Problem: W√∂rter sind nicht direkt vergleichbar
-   wir k√∂nnen erstmal nur z√§hlen, aber nicht vergleichen
-   Beispiele
    -   Bank kann Kreditinstitut bedeuten, in anderem Satz Parkbank
    -   Haus und Geb√§ude sehr √§hnlich, aber nicht reflektiert in Oberfl√§chenform
-   Das hat das Feld vorangetrieben und Methoden hervorgebracht, mit den man W√∂rter in vergleichen kann

:::

## Bag of words

-   word as arbitrary, discrete numbers
    -   `King = 1, Queen = 2, Man = 3, Woman = 4`
-   intrinsic meaning
-   how are these words similar?

![Vector-representations of words as discrete symbols [@Colyer2016]](../images/word2vec-one-hot.png)

::: notes
-   W√∂rter sind diskrete Symbole, eines ist anderes als jedes andere, in unvergleichbarer Weise
-   Vektorrepr√§sentation f√ºr ganzes Vokubular, f√ºr jedes Wort einen Wert
-   Bedeutung liegt in Wort selbst, wenn wir es nicht kennen verstehen wir es nicht
-   BoW bis 2010 Standard, oft erg√§nzt durch Wortart
:::

## Representing a corpus

:::::: columns
:::: {.column width="50%"}
### A collection of documents

::: l-left-align
1.  `NLP is great. I love NLP.`

2.  `I understand NLP.`

3.  `NLP, NLP, NLP.`
:::
::::

::: {.column width="50%"}
### Document term matrix

|              | `NLP` | `I` | `is` | ***term***           |
|--------------|-------|-----|------|----------------------|
| **Doc 1**    | 2     | 1   | 1    | ...                  |
| **Doc 2**    | 1     | 1   | 0    | ...                  |
| **Doc 3**    | 3     | 0   | 0    | ...                  |
| ***Doc ID*** | ...   | ... | ...  | ***term frequency*** |
:::
::::::

::: notes
-   f√ºr Computer m√ºssen Daten tabularisiert werden f√ºr weitere Verarbeitung
-   Eine Textsammlung in Bag of Words Representation sieht so aus
:::

## "I eat `___` tonight". {data-background="var(--blue)"}

::: notes
-   W√∂rter k√∂nnen aber auch anders definiert werden
    -   hier ein Gedankenexperiment mit einem L√ºckentext
-   Frage an Studis
:::

## "The pizza was `___`." {data-background="var(--blue)"}

::: notes
-   Adjektiv √ºber Geschmack, Aussehen, Konsistenz
:::

## 

> You shall know a word by the company it keeps!
>
> @Firth1957

::: notes
-   kontextuelle Bedeutung statt intrinschische Definition
-   Saussure: Zeichen nur definiert durch andere Zeichen
-   relationale Bedeutung: Objekt ist definiert durch Kontext
-   linguistische Theorie blieb lange ohne technische Implementation
    -   man hat einfach W√∂rter gez√§hlt (intrinsische Definition)
:::

## Formalize the linguistic intuition
### Train a model by

1.  masking words
2.  letting the computer predict the masked words using context

::: notes
-   Diese kontextuelle Definition kann man sich zunutze machen
-   Neues Modell lernt eine neue Repr√§sentation von W√∂rter
:::

## Word embeddings

### word2vec [@Mikolov2013]

-   words as continuous vectors
    -   accounting for similarity between words
-   semantic similarity
    -   `King ‚Äì Man + Woman = Queen`
    -   `France / Paris = Switzerland / Bern`

::: l-multiple
![Single continuous vector per word [@Colyer2016]](../images/word_vectors.png)

![Words as points in a semantic space [@Colyer2016]](../images/word2vec-king-queen-vectors.png)

![Doing arithmetics with words <br>[@Colyer2016]](../images/word2vec-king-queen-composition.png)
:::

::: notes
-   Seit 2013 hat sich alles ver√§ndert
-   vector = list of numbers -\> Punkt in mehrdimensionalem Raum
-   Idee: wenn Wort √§hnlich verwendet wird (d.h. bedeutungs√§hnlich ist), dann √§hnliche Position
-   √ºberraschende Eigenschaft: mit W√∂rter rechnen
    -   Synonyme
    -   Analogien
-   Immer noch ein Problem: Ein Wort hat genau ein Vektor
    -   Was passiert mit mehrdeutigen W√∂rter (z.B. Bank)?
-   Frage wie diese Repr√§sentationen genau gelernt wir nach Pause
:::

## Contextualized word embeddings

### BERT [@Devlin2019]

-   recontextualize static word embedding
    -   word gets different embedding in different contexts
    -   accounting for ambiguity (e.g., `bank`)
-   acquire linguistic knowledge from loads of data
    -   mask random words/phrases in sentences

::: notes
-   alles l√§sst sich embedden (W√∂rter, S√§tze, Paragraphen, Dokumente)
:::

## From embeddings to generation

### Instead of masking, train the model to predict the next word

![Autocompletion on iPhone](../images/iphone_autocomplete.jpg)

::: notes
-   predict next word (Google Search, iPhone --\> screenshot)
-   maskieren des n√§chsten Wortes
:::

# Next word prediction is extremely powerful! üí™
::: notes
- M√§chtiger als es auf den ersten Blick aussieht
:::


## The birth: Universal problem solver
### Any task can be modeled as Text-to-Text

![[@Raffel2020]](../images/t5_tasks.png)

::: notes
-   Universal Inference Machine
-   Links der Input (Prompt), rechts der Output
-   Beispiel: Text/Summary
:::

## Large Language Models (LLM) üí•

-   scale up approach of predicting the next word
    -   bigger models and more data
-   optimize for dialogue instead of prose text
    -   instruction-tuning on many [prompt + answer pairs](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25/viewer/default/train) (summarize, translate, explain etc.)
    -   Reinforcement Learning (RL)

::: notes
-   Aus dieser Idee und methodischen Innovation ist ChatGPT und andere Chatbots entstanden
-   Was hat sich ge√§ndert?
    -   Skalierung, Instruktionen, Automatisierung √ºber Reinforcement
-   RL = Mechanismus f√ºr Selbst-Lernen
    -   Wenn man einmal ein gutes Modell hat, kann man verschiedene Outputs generieren lassen und Menschen bewerten lassen
    -   Anderes Modell entwickeln, das menschliche Bewertungen automatisiert
    -   Loop
-   √∂ffentliche Plattform f√ºhrt zu sehr vielen weiteren Daten
-   **Pause**
:::

# Modern NLP is propelled by data

::: notes
-   Sprung zur√ºck zu den Implikationen der Technologie
:::

## Associations in data

</br>

<p>[¬´]{style="font-family:Courier New,Courier,monospace"}[<code>\_\_\_</code>]{style="color:#e74c3c"} becomes a doctor.¬ª</p>

## Learning patterns from data

![Gender bias of the commonly used language model [BERT](https://pair.withgoogle.com/explorables/fill-in-the-blank) [@Devlin2019]](../images/bert_bias_doctor.png)

::: notes
-   BERT wird in Google Search gebraucht
-   Was ist das Problem? Gender Bias
:::

## Cultural associations in training data

![Gender bias of the commonly used language model [BERT](https://pair.withgoogle.com/explorables/fill-in-the-blank) [@Devlin2019]](../images/bert_bias_hobbies.png){width="70%"}

::: notes
-   Analyse umkehren: Nicht nach Pronomen fragen, sondern nach T√§tigkeiten
-   Model trained on Wikipedia and Books (not Reddit).
    -   gesellschaftlicher Fokus, so wie wir √ºber M√§nner und Frauen schreiben
:::

## Word embeddings are biased ...

### ... because ~~our data is~~ we are biased. [@Bender2021]

::: notes
-   Daten sind nicht besser als wir und Gesellschaft tr√§gt extreme Diskriminierungen mit sich
-   Mit Technologie wird Bias aber wieder systematisch in Gesellschaft zur√ºckgef√ºhrt
:::

## Bias with practical implications

![Gender bias in [Google Translate](https://translate.google.com/?sl=en&tl=de&text=Your%20flatmate%20is%20smart.%0AYour%20flatmate%20is%20beautiful.%0A%0AThe%20engineer%20gets%20a%20promotion.%0AThe%20child%20carer%20goes%20to%20the%20zoo%20with%20the%20kids.%0AThe%20child%20carer%20gets%20a%20promotion.%0A%0A&op=translate)](../images/google_translate_bias.png){width="40%"}

::: notes
-   Stereotypen stecken auch in Produkten
-   Bilder oder Websites werden dadurch m√∂glicherweise schlechter gefunden
:::

## In-class: Exercises I {data-background="var(--blue)"}

1.  Open the following website in your browser: <https://pair.withgoogle.com/explorables/fill-in-the-blank/>
2.  Read the the article and play around with the interactive demo.
3.  What works surprisingly well? What looks flawed by societal bias?



## 

> Raw data is an oxymoron.
>
> @Gitelman2013

::: notes
-   Was sind Daten?
    -   Daten sind kein Abbild der Welt, nichts nat√ºrliches.
-   Analog zu Romanos: Massenmedien sind nicht die Welt
    -   Daten sind auch nicht die Welt
-   Daten liegen nicht einfach herum, sondern gemacht
-   Die Frage was Daten genau sind hat mit Realismus/Konstruktivismus-Debatte zu tun
    -   Konstruktivmus heisst nur, Fragen zu stellen, wieso die Dinge sind, wie sie ausschauen
:::

## Fair is a fad

-   companies also engage in fair AI to avoid regulation
-   **Fair** and **good** ‚Äì **but to whom?** [@Kalluri2020]
-   lacking democratic legitimacy

::: notes
-   Fair kann ziemlich vieles bedeuten, solange man es selbst definieren kann
    -   demokratische Legitimit√§t fehlt f√ºr all diese Systeme
-   looking beyond data
    -   invading privacy
    -   economic monopolies
    -   (unpaid) AI-trainers and click-workers
    -   environmental costs
-   Aufpassen, dass wir Diskriminierung nicht automatisieren

Don‚Äôt ask if artificial intelligence is good or fair, ask how it shifts power.
@Kalluri2020
:::

# Modern AI = DL

## How does deep learning model look like?

![Simplified illustration of a neural network. Arrows are weights and bullets are (intermediate) states.](../images/neural_network.png)

::: notes
- Reinf√ºttern von Daten in neurales Netzes und Antwort rausbekommen
-   Ausgangslage sind Input-Output-Paare: 
    -   Input: Bereits generierte W√∂rter (Kontext)
    -   Output: N√§chstes Wort
-   Gewichte werden miteinander multipliziert, √§hnlich wie bei Regression, aber √ºber viele Schichten
-   Modell muss Beziehung zwischen Input/Output zu lernen
    -   Wie kann das Modell lernen?
:::

## How does deep learning work? 

::: {#first-part}
### Deep learning [works](https://thinc.ai/docs/backprop101) like a huge bureaucracy üè¢

1.  **start** with **random** predictions
2.  **blame** units for contributing to **wrong predictions**
3.  **adjust** units based on the accounted blame
4.  **repeat** the cycle
:::

. . .

ü§ì train with **gradient descent**, a series of **small steps** taken **to minimize an error function**

## Current state of deep learning

::: {#first-part}
### Extremely powerful but ... [@Bengio2021]
-   great at **learning patterns**, yet reasoning is still in its infancy
-   requires tons of data due to inefficient learning
-   generalizes poorly
:::

. . .

üëâ avoid anthropomorphizing

::: notes
-   Wir alle sind irgendwie beeindruckt ob den Fortschritten
-   Verbesserung Reasoning
    -   DeepSeek
    -   interner Monolog bevor finaler Output
-   out of domain (schwarze vs weisse Menschen, anderes Textgenre)
-   Vermenschlichung ist nicht angesagt
    - Maschinen funktionieren anders als Menschen un haben andere St√§rken und Schw√§chen
    - the brain does more than predict (sagan)
:::

# Questions? {.white-text data-background-image="../images/paint-anna-kolosyuk-unsplash.jpg"}


## Additional resources

-   deepen your understanding of modern LLMs with

    -   [this video](https://www.youtube.com/watch?v=wjZofJX0v4M)

    -   [this animated explainer](https://ig.ft.com/generative-ai/)
-   racial bias in [CV screening](https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination) and [generated pictures](https://www.bloomberg.com/graphics/2023-generative-ai-bias/) (interactive stories by Bloomberg)

## Optional Exercise I: Limitations of DL {data-background="var(--blue)"}
### Consider the following translation:

<br>

::: {style="font-family:Courier New,Courier,monospace"}
"This sentence contains [37]{style="color:#3498db"} characters."<br /> "Dieser Satz enth√§lt [32]{style="color:#e74c3c"} Buchstaben."<br />
:::

## Optional Exercise I: Limitations of DL
### Reasoning improved, yet still wrong

::::: columns
::: {.column}
![Test April 2024](../images/chatgpt_sentence_length.png){heigth="6cm"}
:::

::: {.column}
![Test April 2025](../images/chatgpt_sentence_length_2025.png){width=60%}
:::
:::::



::: notes
-   Modell hat M√ºhe Intention zu erfassen und Zeichen zu z√§hlen.
-   Auch ohne √úbersetzen funktionierts nicht. 
:::


## References {.scrollable}